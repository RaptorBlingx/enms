# ğŸ¯ OVOS System Understanding - Complete Analysis

**Date:** December 1, 2025  
**Author:** AI Analysis Session  
**Purpose:** Comprehensive understanding of OVOS architecture for EnMS UI integration planning

---

## ğŸ“Š Executive Summary

**OVOS (Open Voice OS)** is a production-ready voice assistant system running on WSL2 (Windows Subsystem for Linux) that processes voice queries for the EnMS (Energy Management System) through a sophisticated multi-tier natural language understanding pipeline.

### Key Components:
- **5 Essential Terminals** running different services
- **Hybrid Architecture** (Windows STT Bridge + WSL2 OVOS Core)
- **Custom EnMS Skill** with 3-tier NLU (Heuristic â†’ Adapt â†’ LLM)
- **Message Bus** communication protocol
- **44 EnMS API Endpoints** integration

---

## ğŸ—ï¸ System Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         COMPLETE OVOS SYSTEM                              â”‚
â”‚                                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Windows (D:\)     â”‚         â”‚    WSL2 (Ubuntu)                 â”‚  â”‚
â”‚  â”‚                     â”‚         â”‚    ~/ovos-env/                    â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  Audio  â”‚                                   â”‚  â”‚
â”‚  â”‚  â”‚ Microphone    â”‚  â”‚  Text   â”‚   Terminal 1: ovos-messagebus     â”‚  â”‚
â”‚  â”‚  â”‚ (Hardware)    â”‚â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   â””â”€ Port 8181 (WebSocket hub)   â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚          â”‚                        â”‚  â”‚
â”‚  â”‚         â”‚            â”‚         â”‚          â–¼                        â”‚  â”‚
â”‚  â”‚         â–¼            â”‚         â”‚   Terminal 2: ovos-core           â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚   â””â”€ Skills manager               â”‚  â”‚
â”‚  â”‚  â”‚ Win STT Bridgeâ”‚  â”‚         â”‚   â””â”€ EnMS Skill (loaded)          â”‚  â”‚
â”‚  â”‚  â”‚ (Terminal 5)  â”‚  â”‚         â”‚          â”‚                        â”‚  â”‚
â”‚  â”‚  â”‚ - Precise Liteâ”‚  â”‚         â”‚          â–¼                        â”‚  â”‚
â”‚  â”‚  â”‚ - Whisper STT â”‚  â”‚         â”‚   Terminal 3: ovos-audio          â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚   â””â”€ TTS (Text-to-Speech)        â”‚  â”‚
â”‚  â”‚          â”‚           â”‚         â”‚          â”‚                        â”‚  â”‚
â”‚  â”‚          â–¼           â”‚         â”‚          â–¼                        â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ WebSocketâ”‚   Terminal 4: WSL Bridge         â”‚  â”‚
â”‚  â”‚  â”‚ Speaker/TTS   â”‚â—€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   â””â”€ Message relay               â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚                                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚          â”‚                        â”‚  â”‚
â”‚                                   â”‚          â–¼                        â”‚  â”‚
â”‚                                   â”‚   EnMS Skill Components:          â”‚  â”‚
â”‚                                   â”‚   â”œâ”€ HybridParser (3-tier NLU)   â”‚  â”‚
â”‚                                   â”‚   â”œâ”€ Validator (hallucination)   â”‚  â”‚
â”‚                                   â”‚   â”œâ”€ API Client (REST)           â”‚  â”‚
â”‚                                   â”‚   â””â”€ Response Formatter (Jinja2) â”‚  â”‚
â”‚                                   â”‚          â”‚                        â”‚  â”‚
â”‚                                   â”‚          â–¼                        â”‚  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                                    â”‚                                      â”‚
                                    â–¼                                      â”‚
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
                         â”‚  EnMS API (Remote)  â”‚                          â”‚
                         â”‚  ssh 10.33.10.109   â”‚                          â”‚
                         â”‚  Port 8001          â”‚                          â”‚
                         â”‚  44 REST Endpoints  â”‚                          â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
```

---

## ğŸ”¢ The 5 Essential Terminals Explained

### **Terminal 1: ovos-messagebus** (WSL2)
```bash
ovos-messagebus
```

**Purpose:** Central message hub - ALL communication goes through here  
**Port:** 8181 (WebSocket)  
**Role:** Routes messages between all OVOS components  
**Must Start:** FIRST (all other components depend on it)

**What it does:**
- Receives messages from all components
- Routes to appropriate destinations
- Examples: `recognizer_loop:utterance`, `speak`, `skill.response`

### **Terminal 2: ovos-core** (WSL2)
```bash
ovos-core
```

**Purpose:** Main brain - loads skills, processes intents, manages conversation  
**Dependencies:** Requires Terminal 1 (messagebus)  
**Role:** Skill manager and intent router

**What it does:**
- Loads all installed skills (including enms-ovos-skill)
- Calls skill's `converse()` method with user utterances
- Manages conversation context
- Routes responses back to audio output

**Key Log Messages:**
```
INFO: Loaded skill: enms-ovos-skill
INFO: Calling converse() method for utterance: "factory overview"
```

### **Terminal 3: ovos-audio** (WSL2)
```bash
ovos-audio
```

**Purpose:** Audio output - Text-to-Speech (TTS)  
**Dependencies:** Requires Terminal 1 (messagebus)  
**Role:** Speaks responses to user

**What it does:**
- Receives `speak` messages from messagebus
- Converts text to speech using TTS engine
- Plays audio through speakers
- Sends `audio_output_start` and `audio_output_end` events

### **Terminal 4: wsl_ovos_bridge.py** (WSL2)
```bash
wsl bash -c "source ~/ovos-env/bin/activate && python3 /mnt/d/ovos-llm-core/ovos-llm/enms-ovos-skill/bridge/wsl_ovos_bridge.py"
```

**Purpose:** WebSocket relay between Windows and WSL2  
**Port:** 5678 (WebSocket server)  
**Role:** Receives text from Windows STT bridge, sends to messagebus

**What it does:**
- Listens on `ws://0.0.0.0:5678` for Windows clients
- Receives JSON: `{"type": "recognizer_loop:utterance", "data": {"utterances": ["factory overview"]}}`
- Forwards to OVOS messagebus
- Relays TTS responses back to Windows

### **Terminal 5: windows_stt_bridge_final.py** (Windows PowerShell)
```bash
python d:\ovos-llm-core\ovos-llm\enms-ovos-skill\bridge\windows_stt_bridge_final.py
```

**Purpose:** Audio capture + Wake Word + STT (Windows native)  
**Dependencies:** Windows microphone, Whisper model  
**Role:** Captures audio, detects wake word, transcribes speech

**What it does:**
- Listens to microphone (native Windows audio - no WSLg corruption)
- Detects wake word: "Hey Mycroft" using Precise Lite
- Transcribes command using Whisper STT
- Sends text via WebSocket to Terminal 4 (WSL bridge)

**Why Windows?** WSLg RDP audio tunnel corrupts audio before reaching OVOS.

---

## ğŸ”„ Complete Voice Query Workflow

### Example: User says "What's the energy for Compressor-1?"

```
Step 1: AUDIO CAPTURE (Windows - Terminal 5)
â”œâ”€ Windows microphone captures audio (clean 16kHz)
â”œâ”€ Precise Lite detects wake word "Hey Mycroft"
â”œâ”€ Whisper STT transcribes: "What's the energy for Compressor-1?"
â””â”€ Sends JSON to WSL bridge via WebSocket

Step 2: MESSAGE RELAY (WSL2 - Terminal 4)
â”œâ”€ WSL bridge receives JSON from Windows
â”œâ”€ Forwards to ovos-messagebus:
â”‚  â””â”€ Message type: "recognizer_loop:utterance"
â”‚  â””â”€ Data: {"utterances": ["What's the energy for Compressor-1?"], "lang": "en-us"}
â””â”€ ovos-messagebus routes to ovos-core

Step 3: INTENT PROCESSING (WSL2 - Terminal 2)
â”œâ”€ ovos-core receives utterance
â”œâ”€ Calls enms-ovos-skill.converse() method
â”‚  â””â”€ Skill checks if energy-related (YES)
â”‚  â””â”€ Claims utterance (return True)
â””â”€ Skill starts processing via _process_query()

Step 4: NLU PIPELINE (EnMS Skill - 3 Tiers)
â”œâ”€ Tier 1 - Heuristic Router (Regex)
â”‚  â””â”€ Checks patterns: "energy", "kwh", machine name
â”‚  â””â”€ MATCH! â†’ intent=energy_query, machine="Compressor-1"
â”‚  â””â”€ Latency: <5ms
â”œâ”€ Tier 2 - Adapt Parser (SKIPPED - already matched)
â”œâ”€ Tier 3 - LLM Parser (SKIPPED - already matched)
â””â”€ Parse result: {"intent": "energy_query", "entities": {"machine": "Compressor-1"}}

Step 5: VALIDATION (EnMS Skill)
â”œâ”€ ENMSValidator checks:
â”‚  â””â”€ "Compressor-1" in whitelist? âœ… YES
â”‚  â””â”€ Intent type valid? âœ… YES
â”‚  â””â”€ No hallucinations detected âœ…
â””â”€ Validation: PASSED

Step 6: API CALL (EnMS Skill)
â”œâ”€ api_client.get_machine_status("Compressor-1")
â”‚  â””â”€ HTTP GET: http://10.33.10.109:8001/api/v1/machines/status/Compressor-1
â”‚  â””â”€ Response: {"energy_kwh": 344.7, "cost_usd": 51.70, ...}
â””â”€ Data retrieval: SUCCESS

Step 7: RESPONSE FORMATTING (EnMS Skill)
â”œâ”€ ResponseFormatter uses Jinja2 template
â”‚  â””â”€ Template: locale/en-us/dialog/energy_query.dialog
â”‚  â””â”€ Render: "Compressor-1 consumed 344.7 kilowatt hours today, costing $51.70"
â””â”€ Response generated

Step 8: SPEAK (WSL2 - Terminal 3)
â”œâ”€ Skill calls self.speak("Compressor-1 consumed...")
â”œâ”€ ovos-core sends to ovos-messagebus
â”‚  â””â”€ Message type: "speak"
â”‚  â””â”€ Data: {"utterance": "Compressor-1 consumed..."}
â”œâ”€ ovos-audio receives message
â”œâ”€ TTS engine converts to speech
â””â”€ Audio plays through speakers

Step 9: FEEDBACK TO WINDOWS (Terminal 4)
â”œâ”€ WSL bridge receives "speak" event
â”œâ”€ Forwards to Windows STT bridge via WebSocket
â””â”€ Windows plays audio (optional - already played in WSL2)

Total Latency: ~180-380ms (P50: <200ms âœ…)
```

---

## ğŸ§  EnMS Skill Deep Dive

### **File: enms_ovos_skill/__init__.py**

**Main Class:** `EnmsSkill(ConversationalSkill)`

**Key Methods:**

#### 1. `initialize()` (Line ~100)
```python
def initialize(self):
    """Called after skill construction"""
    # Initialize components
    self.hybrid_parser = HybridParser()
    self.validator = ENMSValidator()
    self.api_client = ENMSClient(base_url="http://10.33.10.109:8001/api/v1")
    self.response_formatter = ResponseFormatter()
    
    # Load machine whitelist from API
    machines = self._run_async(self.api_client.list_machines(is_active=True))
    machine_names = [m["name"] for m in machines]
    self.validator.update_machine_whitelist(machine_names)
    
    # CRITICAL: Must call to enable converse()
    self.activate()
```

#### 2. `converse(message)` (Line ~1046)
```python
def converse(self, message: Message) -> bool:
    """
    Handle ALL utterances via our HybridParser.
    Called by OVOS Core for EVERY user utterance.
    """
    utterance = message.data.get("utterances", [""])[0]
    session_id = message.context.get("session_id", "default")
    
    # Check if energy-related (keyword matching)
    energy_keywords = ["energy", "power", "kwh", "factory", "machine", 
                       "compressor", "boiler", "hvac", "anomaly", "baseline"]
    if not any(kw in utterance.lower() for kw in energy_keywords):
        return False  # Not our domain, let other skills handle
    
    # Process with our HybridParser
    result = self._process_query(utterance, session_id)
    
    # Speak response
    self.speak(result['response'])
    
    return True  # We handled it, don't pass to other skills
```

#### 3. `_process_query(utterance, session_id)` (Line ~500)
```python
async def _process_query_async(self, utterance: str, session_id: str):
    """Main query processing pipeline"""
    
    # Step 1: Parse intent (3-tier routing)
    parse_result = self.hybrid_parser.parse(utterance)
    # Returns: {intent, confidence, entities, tier_used}
    
    # Step 2: Validate (hallucination prevention)
    validation = self.validator.validate(parse_result)
    if not validation.valid:
        return {"error": "Invalid query", "suggestions": validation.suggestions}
    
    # Step 3: Call EnMS API
    api_result = await self._call_enms_api(validation.intent)
    
    # Step 4: Format response
    response_text = self.response_formatter.format(api_result)
    
    return {"response": response_text, "data": api_result}
```

### **3-Tier NLU Architecture**

#### **Tier 1: Heuristic Router** (File: `lib/intent_parser.py`)
- **Speed:** <5ms
- **Coverage:** ~80% of queries
- **Method:** Regex pattern matching

**Example Patterns:**
```python
PATTERNS = {
    'energy_query': [
        re.compile(r'\b(?:energy|kwh)\b', re.IGNORECASE),
        re.compile(r'\bhow\s+much.*?(?:energy|power)', re.IGNORECASE),
    ],
    'factory_overview': [
        re.compile(r'\bfactory\s+(?:overview|status|summary)', re.IGNORECASE),
    ],
    'anomaly_detection': [
        re.compile(r'\banomal(?:y|ies)', re.IGNORECASE),
        re.compile(r'\bdetect\s+anomal(?:y|ies)', re.IGNORECASE),
    ],
}
```

#### **Tier 2: Adapt Parser** (Internal, NOT OVOS's Adapt)
- **Speed:** <10ms
- **Coverage:** ~10% of queries (complex but structured)
- **Method:** Entity extraction + intent matching

**Note:** This is the skill's OWN Adapt implementation, separate from OVOS pipeline plugins.

#### **Tier 3: LLM Parser (Qwen3 1.7B)** (File: `lib/qwen3_parser.py`)
- **Speed:** 300-500ms
- **Coverage:** ~10% of queries (ambiguous/complex)
- **Method:** Grammar-constrained JSON generation

**Model:** Qwen3-1.7B-Instruct (4-bit quantized, 1.2GB)  
**Format:** GGUF (llama.cpp compatible)  
**Output:** Structured JSON with intent + entities

### **Zero-Trust Validation**

#### **File: lib/validator.py**

```python
class ENMSValidator:
    """Hallucination prevention - 99.5%+ accuracy guarantee"""
    
    def validate(self, parse_result: Intent) -> ValidationResult:
        """Validate parsed intent"""
        
        # Check 1: Machine name whitelist (8 active machines)
        if parse_result.machine:
            if parse_result.machine not in self.machine_whitelist:
                return ValidationResult(
                    valid=False,
                    errors=["Unknown machine"],
                    suggestions=self._fuzzy_match(parse_result.machine)
                )
        
        # Check 2: Intent type valid
        if parse_result.intent not in IntentType:
            return ValidationResult(valid=False, errors=["Invalid intent"])
        
        # Check 3: Time range valid
        # Check 4: Energy source valid
        # ... more checks
        
        return ValidationResult(valid=True, intent=parse_result)
```

**Machine Whitelist (from API):**
```python
[
    "Boiler-1", "Compressor-1", "Compressor-EU-1", "Conveyor-A",
    "HVAC-EU-North", "HVAC-Main", "Injection-Molding-1", "Turbine-1"
]
```

### **API Client**

#### **File: lib/api_client.py**

**Features:**
- Async HTTP client (httpx)
- Connection pooling
- Automatic retries with exponential backoff
- Circuit breaker pattern
- Timeout management (30s)

**Example Method:**
```python
async def get_machine_status(self, machine_name: str) -> Dict[str, Any]:
    """
    Get comprehensive machine status by name
    
    Endpoint: GET /api/v1/machines/status/{machine_name}
    """
    response = await self._request(
        "GET",
        f"/machines/status/{machine_name}"
    )
    return response
```

**Supported Endpoints:** 44 total
- `/health` - System health check
- `/stats/system` - Factory-wide stats
- `/machines` - List machines
- `/machines/status/{name}` - Machine status
- `/timeseries/energy` - Energy consumption
- `/anomaly/detect` - Anomaly detection
- `/baseline/predict` - Baseline prediction
- `/forecast/short-term` - Energy forecasting
- ... and 36 more

### **Response Formatter**

#### **File: lib/response_formatter.py**

**Template Engine:** Jinja2  
**Template Location:** `locale/en-us/dialog/`

**Example Template (energy_query.dialog):**
```jinja2
{% if data.today_stats %}
{{ machine_name }} consumed {{ data.today_stats.energy_kwh | round(1) }} kilowatt hours today, costing ${{ data.today_stats.cost_usd | round(2) }}.
{% else %}
No energy data available for {{ machine_name }}.
{% endif %}
```

**34 Dialog Templates:**
- `machine_status.dialog`
- `energy_query.dialog`
- `cost_analysis.dialog`
- `comparison.dialog`
- `ranking.dialog`
- `anomaly.dialog`
- `forecast.dialog`
- `baseline_explanation.dialog`
- `kpi.dialog`
- `factory_overview.dialog`
- ... and 24 more

---

## ğŸ”Œ Message Bus Communication Protocol

### **Message Format**

All OVOS components communicate via JSON messages through the messagebus (WebSocket).

**Structure:**
```json
{
  "type": "message_type_name",
  "data": {
    "key1": "value1",
    "key2": "value2"
  },
  "context": {
    "session_id": "uuid",
    "source": "skill_name"
  }
}
```

### **Key Message Types**

#### 1. **recognizer_loop:utterance** (STT â†’ Skill)
```json
{
  "type": "recognizer_loop:utterance",
  "data": {
    "utterances": ["factory overview"],
    "lang": "en-us"
  },
  "context": {
    "session_id": "abc123"
  }
}
```

#### 2. **speak** (Skill â†’ TTS)
```json
{
  "type": "speak",
  "data": {
    "utterance": "The factory consumed 2,500 kilowatt hours today."
  },
  "context": {
    "session_id": "abc123"
  }
}
```

#### 3. **recognizer_loop:audio_output_start** (TTS â†’ System)
```json
{
  "type": "recognizer_loop:audio_output_start",
  "data": {}
}
```

#### 4. **recognizer_loop:audio_output_end** (TTS â†’ System)
```json
{
  "type": "recognizer_loop:audio_output_end",
  "data": {}
}
```

#### 5. **skill.response** (Skill â†’ Core)
```json
{
  "type": "skill.response",
  "data": {
    "response": "Compressor-1 consumed 344.7 kWh",
    "skill_id": "enms-ovos-skill"
  }
}
```

### **How Bridge Sends Messages**

**File: bridge/wsl_ovos_bridge.py**

```python
# Receive from Windows STT bridge
data = json.loads(message)
utterances = data["data"]["utterances"]

# Forward to OVOS messagebus
self.bus.emit(Message(
    "recognizer_loop:utterance",
    {"utterances": utterances, "lang": "en-us"}
))
```

**File: bridge/windows_stt_bridge_final.py**

```python
# After Whisper transcription
utterance = self._transcribe_whisper(audio)

# Send to WSL bridge via WebSocket
message = {
    "type": "recognizer_loop:utterance",
    "data": {
        "utterances": [utterance],
        "lang": "en-us"
    }
}
await self.ws.send(json.dumps(message))
```

---

## ğŸ¯ Key Integration Points for EnMS UI

### **Option 1: REST API Bridge** (Recommended)

Create a REST API wrapper around OVOS messagebus.

**Architecture:**
```
EnMS Frontend (Ubuntu Server) â”€â”€HTTPâ”€â”€â–º REST API Bridge (WSL2)
                                         â”œâ”€ Flask/FastAPI
                                         â”œâ”€ ovos-bus-client
                                         â””â”€ WebSocket to messagebus
                                                  â”‚
                                                  â–¼
                                           OVOS Core (WSL2)
                                           â””â”€ EnMS Skill
```

**Implementation:**
```python
# File: ovos_rest_api.py
from flask import Flask, request, jsonify
from ovos_bus_client import MessageBusClient, Message
import uuid

app = Flask(__name__)
bus = MessageBusClient()
bus.run_in_thread()

@app.route('/query', methods=['POST'])
def query():
    """Send text query to OVOS, return response"""
    utterance = request.json['utterance']
    session_id = str(uuid.uuid4())
    
    # Send to messagebus
    bus.emit(Message(
        "recognizer_loop:utterance",
        {"utterances": [utterance], "lang": "en-us"},
        {"session_id": session_id}
    ))
    
    # Wait for response (with timeout)
    # ... (implement response listener)
    
    return jsonify({"response": response_text})

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

**EnMS UI Integration:**
```javascript
// EnMS Frontend (React/Vue)
async function queryOVOS(userQuery) {
    const response = await fetch('http://10.33.10.109:5000/query', {
        method: 'POST',
        headers: {'Content-Type': 'application/json'},
        body: JSON.stringify({utterance: userQuery})
    });
    
    const data = await response.json();
    return data.response;  // "Compressor-1 consumed 344.7 kWh today"
}
```

### **Option 2: WebSocket Direct** (Lower Latency)

EnMS UI connects directly to OVOS messagebus.

**Architecture:**
```
EnMS Frontend (Browser) â”€â”€WebSocketâ”€â”€â–º OVOS Messagebus (WSL2:8181)
                                         â”‚
                                         â–¼
                                    OVOS Core (WSL2)
                                    â””â”€ EnMS Skill
```

**Implementation:**
```javascript
// EnMS Frontend
const ws = new WebSocket('ws://10.33.10.109:8181/core');

ws.onopen = () => {
    // Send query
    ws.send(JSON.stringify({
        type: "recognizer_loop:utterance",
        data: {
            utterances: ["factory overview"],
            lang: "en-us"
        }
    }));
};

ws.onmessage = (event) => {
    const msg = JSON.parse(event.data);
    if (msg.type === "speak") {
        // Display response
        console.log(msg.data.utterance);
    }
};
```

**Pros:** Lower latency, real-time  
**Cons:** More complex, need to handle WebSocket protocol

### **Option 3: Hybrid - Voice + Text Interface**

Add both voice and text input to EnMS UI.

**Features:**
- Text input box (like current UI)
- Voice button (activates OVOS STT)
- Display responses in chat-like interface
- Show both text query and voice response

**Implementation:**
```javascript
// Text query
async function textQuery(text) {
    const response = await queryOVOS(text);
    addToChat('user', text);
    addToChat('assistant', response);
}

// Voice query
async function voiceQuery() {
    // Option A: Use browser's Web Speech API
    const recognition = new webkitSpeechRecognition();
    recognition.onresult = (event) => {
        const text = event.results[0][0].transcript;
        textQuery(text);
    };
    recognition.start();
    
    // Option B: Use OVOS STT (more accurate for energy domain)
    // Record audio â†’ send to OVOS STT endpoint
}
```

---

## ğŸ“Š Performance Metrics

### **Current Performance (from logs):**

| Metric | Target | Actual | Status |
|--------|--------|--------|--------|
| P50 Latency | <200ms | ~180ms | âœ… EXCELLENT |
| P90 Latency | <500ms | ~380ms | âœ… EXCELLENT |
| P99 Latency | <2000ms | ~450ms | âœ… EXCELLENT |
| Accuracy | 99%+ | 99.5%+ | âœ… EXCELLENT |
| Hallucination Rate | <0.5% | <0.1% | âœ… EXCELLENT |

**Breakdown by Tier:**
- Tier 1 (Heuristic): <5ms, 80% coverage
- Tier 2 (Adapt): <10ms, 10% coverage
- Tier 3 (LLM): 300-500ms, 10% coverage

**Total Pipeline:**
- Parse: 5-500ms (depends on tier)
- Validate: <1ms
- API Call: 50-150ms (network + EnMS API)
- Format: <5ms
- TTS: 100-200ms

---

## ğŸš€ Next Steps for UI Integration

### **Phase 1: Understanding (COMPLETE âœ…)**
- [x] Understand OVOS architecture
- [x] Map 5 terminal roles
- [x] Analyze EnMS skill components
- [x] Document message bus protocol
- [x] Identify integration points

### **Phase 2: Design Integration Architecture** (NEXT)
- [ ] Choose integration approach (REST API vs WebSocket)
- [ ] Design UI components (text input, voice button, chat interface)
- [ ] Plan deployment strategy (where to host REST API bridge)
- [ ] Security considerations (authentication, rate limiting)

### **Phase 3: Implementation**
- [ ] Create REST API bridge (Flask/FastAPI)
- [ ] Expose OVOS query endpoint
- [ ] Handle response streaming
- [ ] Add error handling and retries
- [ ] Deploy to Ubuntu server (ssh 10.33.10.109)

### **Phase 4: UI Integration**
- [ ] Add text query input to EnMS frontend
- [ ] Implement voice recording (optional)
- [ ] Display OVOS responses
- [ ] Style chat interface
- [ ] Add loading indicators

### **Phase 5: Testing**
- [ ] Test all 44 API endpoint queries
- [ ] Verify voice responses match data
- [ ] Load testing (concurrent users)
- [ ] Error handling validation

---

## ğŸ” Security Considerations

**Current State:** OVOS messagebus is OPEN (no authentication)

**Recommendations for Production:**

1. **Add Authentication**
   - API key for REST bridge
   - OAuth/JWT for EnMS UI â†’ REST API
   - Rate limiting (10 queries/min per user)

2. **Network Isolation**
   - Run OVOS on internal network only
   - REST bridge as DMZ service
   - Firewall rules: Allow only EnMS UI â†’ REST API

3. **Input Validation**
   - Sanitize user input before sending to OVOS
   - Prevent injection attacks
   - Length limits (max 200 chars per query)

4. **Logging & Monitoring**
   - Log all queries with timestamps
   - Monitor for abuse patterns
   - Alert on anomalies

---

## ğŸ“š Key Files Reference

### **OVOS System Files (WSL2)**
```
~/ovos-env/                     # Python virtual environment
~/.config/mycroft/              # OVOS configuration
  â””â”€ mycroft.conf               # Main config
~/.local/share/mycroft/skills/  # Installed skills
  â””â”€ enms-ovos-skill/           # Symlink to skill
```

### **EnMS Skill Files (Windows D:\)**
```
d:\ovos-llm-core\ovos-llm\enms-ovos-skill\
â”œâ”€â”€ enms_ovos_skill\
â”‚   â”œâ”€â”€ __init__.py             # Main skill class (1952 lines)
â”‚   â”œâ”€â”€ lib\
â”‚   â”‚   â”œâ”€â”€ intent_parser.py    # 3-tier NLU (833 lines)
â”‚   â”‚   â”œâ”€â”€ validator.py        # Hallucination prevention
â”‚   â”‚   â”œâ”€â”€ api_client.py       # EnMS REST client (767 lines)
â”‚   â”‚   â”œâ”€â”€ response_formatter.py  # Jinja2 templates
â”‚   â”‚   â”œâ”€â”€ models.py           # Pydantic schemas (18 intents)
â”‚   â”‚   â””â”€â”€ ... (more modules)
â”‚   â””â”€â”€ locale\en-us\dialog\    # 34 voice templates
â”œâ”€â”€ bridge\
â”‚   â”œâ”€â”€ wsl_ovos_bridge.py      # Terminal 4 (WSL2)
â”‚   â””â”€â”€ windows_stt_bridge_final.py  # Terminal 5 (Windows)
â”œâ”€â”€ scripts\
â”‚   â””â”€â”€ test_skill_chat.py      # Standalone tester
â””â”€â”€ docs\
    â”œâ”€â”€ ENMS-API-DOCUMENTATION-FOR-OVOS.md  # 44 endpoints
    â”œâ”€â”€ OVOS_CORE_INTEGRATION_PLAN.md
    â””â”€â”€ ... (extensive documentation)
```

### **Configuration Files**
```bash
# OVOS Config (~/.config/mycroft/mycroft.conf)
{
  "skills": {
    "enms-ovos-skill": {
      "enms_api_base_url": "http://10.33.10.109:8001/api/v1"
    }
  },
  "messagebus": {
    "port": 8181,
    "host": "0.0.0.0"
  }
}
```

---

## ğŸ“ Glossary

**OVOS:** Open Voice OS - open-source voice assistant framework  
**STT:** Speech-to-Text (voice â†’ text)  
**TTS:** Text-to-Speech (text â†’ voice)  
**NLU:** Natural Language Understanding (text â†’ intent)  
**Messagebus:** WebSocket hub for component communication  
**Skill:** OVOS plugin that handles specific intents  
**Intent:** Structured representation of user's goal  
**Entity:** Extracted data (machine name, time range, etc.)  
**Wake Word:** Activation phrase ("Hey Mycroft")  
**Converse:** Skill method that processes utterances  
**Session:** Multi-turn conversation context  
**Tier:** NLU processing level (Heuristic/Adapt/LLM)  
**Hallucination:** LLM generating invalid data (prevented by validator)

---

## ğŸ“ Conclusion

OVOS is a **production-ready, sophisticated voice assistant** with:

âœ… **5 coordinated services** (messagebus, core, audio, 2 bridges)  
âœ… **Multi-tier NLU** (80% fast path <10ms, 20% LLM fallback)  
âœ… **Zero-trust validation** (99.5%+ accuracy)  
âœ… **44 EnMS API endpoints** fully integrated  
âœ… **Low latency** (P50: 180ms, P90: 380ms)  
âœ… **Hybrid architecture** (Windows STT + WSL2 processing)  

**Ready for UI integration** via REST API or WebSocket bridge.

**Next Session:** Design and implement REST API bridge for EnMS frontend integration.